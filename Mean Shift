# Mean shift for hierarchical clustering
# mean shift figures out how many clusters there should be
# unlike K-Means, where the individual chooses how many clusters there should be.

# Radius will be created around a datapoint, this is called a datapoints "bandwidth"
# Every cluster center goes through the process of determining a radius and
# whatever feature sets fall within its bandwidth

# It takes the mean or average of all the datapoints radii and moves the overall cluster centroid
# to accomodate for the changing data. This is called Convergence. All datapoints radii and bandwidth approach
# a convergence.

import numpy as np
from sklearn.cluster import MeanShift
from sklearn.datasets.samples_generator import make_blobs
import matplotlib.pyplot as plt
# for 3d graphing
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import style
style.use("ggplot")

#create starting centers
centers = [[1, 2, 12], [4, 5, 11], [7, 8, 10]]
X, _ = make_blobs(n_samples = 100, centers = centers, cluster_std =1)

ms = MeanShift()
ms.fit(X)
labels = ms.labels_
# compare clusters given from mean shifts automated centers
cluster_centers = ms.cluster_centers_
print(cluster_centers)
n_clusters_ = len(np.unique(labels))
print("Number of estimates clusters:", n_clusters_)

#multiply list of colors by 10 to allow for a lot of options
colors = 10*['r','g','b','c','k','y','m']
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

for i in range(len(X)):
    ax.scatter(X[1][0], X[i][1], X[i][2], c=colors[labels[i]], marker='o')

ax.scatter(cluster_centers[:,0],cluster_centers[:,1],cluster_centers[:,2],
           marker='x', color='k', s=150, linewidths = 5,zorder=10)

plt.show()
